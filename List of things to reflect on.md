- [ ] The Craftsman
- [ ] Designing Reality
- [ ] When Things Start to Think
- [x] Fab
- [ ] Arts & Crafts
- [ ] Craftinnova
- [ ] Càtedra Artesania UPV
- [ ] L'artisan electronique
- [x] AI.RTISANSHIP
- [ ] DAFNE+
- [ ] CULTURALITY
- [x] Hives 
- [ ] Manuel Martinez Torán
- [ ] Amandine David
- [ ] FERRAMENTES DIGITALS APLICADES A LA CONSERVACIÓ, GESTIÓ I DIFUSIÓ DEL PATRIMONI CULTURAL
- [ ] Ninot Imprés 3d
- [ ] Made In platform
- [ ] Artesanía digital
- [ ] Artesanía y tecnología. De lo analógico a lo digital




At the intersection of physical science and computer science,
programs can process atoms as well as bits, digitizing fabrication in
the same way that communications and computation were earlier
digitized. Ultimately, this means that a programmable personal
fabricator will be able to make anything, including itself, by assembling
atoms. It will be a self-reproducing machine. That idea has been a
long-standing science fiction staple for better or, sometimes, much
worse.
In Star Trek: The Next Generation, the replicator is an essential plot
element that is capable of making whatever is needed for each
episode. It looks like an overgrown drinks dispenser, but it has the
useful feature of being able to dispense anything. In theory, it does this
by following stored instructions to put together subatomic particles to
make atoms, atoms to make molecules, and molecules to make
whatever you want. For Captain Picard, that was frequently a steaming
mug of his preferred tea, obtained from the replicator with the
command “Tea, Earl Grey, hot.”


The overwhelming interest from students with relatively little technical
experience (for MIT) was only the first surprise. The next was the
reason why they wanted to take the class. Virtually no one was doing
this for research. Instead, they were motivated by the desire to make
things they’d always wanted, but that didn’t exist. These ranged from
practical (an alarm clock that needs to be wrestled into turning off), to
fanciful (a Web browser for parrots), to profoundly quirky (a portable
personal space for screaming). Their inspiration wasn’t professional; it
was personal. The goal was not to publish a paper, or file a patent, or
market a product. Rather, their motivation was their own pleasure in
making and using their inventions.
The third surprise was what these students managed to accomplish.
Starting out with skills more suited to arts and crafts than advanced
engineering, they routinely and single-handedly managed to design
and build complete functioning systems. Doing this entailed creating
both the physical form—mastering the use of computer-controlled tools
that produce three-dimensional shapes by adding or removing
material—and the logical function—designing and building circuits
containing embedded computer chips interfaced with input and output
devices. In an industrial setting these tasks are distributed over whole
teams of people who conceive, design, and produce a product. No
one member of such a team could do all of this, and even if they could,
they wouldn’t: personal screaming technology is unlikely to emerge as
a product plan from a marketing meeting (even if the participants might
secretly long for it).
The final surprise (p.7)



The final surprise was how these students learned to do what they
did: the class turned out to be something of an intellectual pyramid
scheme. Just as a typical working engineer would not have the design
and manufacturing skills to personally produce one of these projects,
no single curriculum or teacher could cover the needs of such a
heterogeneous group of people and machines. Instead, the learning
process was driven by the demand for, rather than supply of,
knowledge. Once students mastered a new capability, such as
waterjet cutting or microcontroller programming, they had a near-
evangelical interest in showing others how to use it. As students
needed new skills for their projects they would learn them from their
peers and then in turn pass them on. Along the way, they would leave
behind extensive tutorial material that they assembled as they worked.
This phase might last a month or so, after which they were so busy
using the tools that they couldn’t be bothered to document anything, but
by then others had taken their place. This process can be thought of as
a “just-in-time” educational model, teaching on demand, rather than the
more traditional “just-incase” model that covers a curriculum fixed in
advance in the hopes that it will include something that will later be
useful.
These surprises have recurred with such certainty year after year
that I began to realize that these students were doing much more than
taking a class; they were inventing a new physical notion of literacy.
The common understanding of “literacy” has narrowed down to reading
and writing, but when the term emerged in the Renaissance it had a
much broader meaning as a mastery of the available means of
expression. However, physical fabrication was thrown out as an
“illiberal art,
” pursued for mere commercial gain. These students were
correcting a historical error, using millions of dollars’ worth of
machinery for technological expression every bit as eloquent as a
sonnet or a painting.
Today there aren’t many places where these kinds of tools are
available for play rather than work, but their capabilities will be
integrated into accessible and affordable consumer versions. Such a
future really represents a return to our industrial roots, before art was
separated from artisans, when production was done for individuals
rather than masses. Life without the infrastructure we take for granted
today required invention as a matter of survival rather than as a
specialized profession. The design, production, and use of engineered
artifacts—agricultural implements, housewares, weapons, and armor
—all took place locally. The purpose of bringing tool-making back into
the home is not to recreate the hardships of frontier living, just as it’s
not to run personalscream-container production lines out of the family
room. Rather, it’s to put control of the creation of technology back in
the hands of its users.
(p. 7&8)

## ~~The Past~~
Once upon a time, education, industry, and art were integrated in the work of a village artisan. By the time that I went to school, collegebound kids like me had to sit in rather sterile classrooms, while the kids taking up trades got to go to a vocational school that had all the cool stuff—machine tools, welders, electronic test equipment, and the like. At the time, this split seemed vaguely punitive for someone like me. I couldn’t understand why an interest in making things was taken as a sign of lesser intelligence. It wasn’t until I became an MIT professor and had an official excuse to buy those kinds of tools that I realized the problem predates my high school by quite a bit. The Kellys and Meejins and Shellys and Dalias of the world are correcting a historical error that dates back to the Renaissance.

In the fifteenth and sixteenth centuries, stiff paintings of religious subjects gave way to much more diverse images of breathtaking realism and luminosity. The advances that made this possible were first of all technical, including the introduction of (originally Flemish) oil paints and the development of mathematical techniques for establishing perspective. Oil-based paints dried more slowly, allowing more complex brushstrokes; the intensity of the colors led to thinner layers that betterreflected light; and the viscosity of the paints improved their coverage on wood and canvas. At the same time, vision studies led to geometrical solutions for the problem of how to best project a three-dimensional scene onto a two-dimensional surface. These techniques were created by and for artists; Leonardo da Vinci, for example, was continually experimenting with new paints.

Improvements in paints and perspective would not have had the impact they did if not for the simultaneous development of artists to use them. Your average Middle Ages painter worked as an artisan in a guild, with the same (lack of) creative control as a carpenter. An aspiring painter would progress from apprentice to journeyman to master, finally gaining full admission to the guild by producing a

“masterpiece.” The real mastery of the guilds, however, was of the marketplace: they were very effective monopolists, controlling both the supply of and standards for skilled laborers. The work itself was done to detailed specifications drawn up by, say, a church that wanted an altarpiece illustrated with a particular scene.

The guild system began to break down under both the fragmentation of the crafts into increasingly narrow specialties and their aggregation into workshops that could produce increasingly complex, complete artifacts (and which formed the seeds for the later development of factories). But creative individuals were able to escape from the guild system because of another invention: customers. Artisans became artists when a population emerged that had both the discretionary income and the intellectual interest to acquire art.

Led by merchant families, most notably the Medicis in Florence, and the occasional municipality or pre-Enlightened monarch, a community of patrons began to emerge that bought art from and for individuals rather than (literally) dogmatic institutions. Michelangelo and Leonardo da Vinci started their careers as apprentices but ended up valued for their personal expression rather than their occupational productivity. Leonardo da Vinci ultimately represented just himself. He was not the CEO of a nascent da Vinci Industries, with a fiduciary responsibility to its shareholders to maximize the brand’s value (which is a good thing, otherwise his legacy might instead have been a line of Mona Lisa and Luigi dolls).

As remarkable as these new artists and their new materials and methods for painting were, their subject material was more significant still. Paintings began to represent the world of people rather than that of myths or gods. This human focus both reflected and shaped the defining intellectual movement of the Renaissance: humanism. What liberal arts students now study as the humanities emerged in the Renaissance as an expression of a growing sense of mastery by (selected) people over their world.

In Italy, humanism grew in part out of a fifteenth-century attempt to understand Roman and Greek ruins, both architectural and intellectual, an effort that today would be called reverse engineering. While much ancient knowledge and experience had been lost in the intervening

centuries, the existing record of ruins and ancient documents provided a handy template for reconstructing a society that had worked pretty well for the Romans (other than the decline-and-fall issue). This enterprise benefited considerably when the Ottomans conquered Constantinople in 1453, freeing up a ready supply of fleeing Greek scholars. When they sought refuge in Italy they brought with them writings and knowledge that had long been lost to the West.

A second ingredient in the emergence of humanism arose as a reaction against the strictures and scriptures of the prevalent monastic, ecclesiastical seat of advanced education. While nominally still religiously observant, the growing urban mercantile economy and rule by civic authority brought a need for training future secular leaders with relevant skills. The human part ofhumanism comes fromstudia humanitatis (“studies of humanity”), referring to this shift in focus from immortal to mortal subjects, and was associated with a growing interest in how people were reflected in everything from portraiture to pedagogy.

These threads of humanism came together with the dusting off (sometimes quite literally) of a curriculum based around the four-part quadrivium (geometry, arithmetic, astronomy, music) and the threepart trivium (grammar, logic, rhetoric). These Latin terms refer back to four- and three-way road intersections, the latter notable as a place where people would congregate and share knowledge that naturally came to be known as trivial, or trivia. The trivium and the quadrivium together make up the seven “liberal arts.” Both of these words warrant comment. “Liberal” in this sense is not the opposite of “conservative”; it referred to the liberation that the study of these subjects was thought to bring. And “art” did not mean just creative expression; it meant much more broadly the mastery that was developed over each of these domains. Liberal arts originally has this rather rousing meaning as a mastery over the means for personal liberation. They’re now associated with academic study that is remote from applications, but they emerged in the Renaissance as a humanist pathway to power.

In 1513 Niccolò Machiavelli wrote The Prince, the classic (and infamous) guide to governance, on how to use rhetoric to win friends and manipulate people. Renaissance social engineering also gave

birth to the concept of utopia, if not the reality. The term first appeared in a book of that name, written by Sir Thomas More in 1516; his utopian vision was very much a humanist paradise, governed by reason and based on a belief in the power of ideas. It was against this backdrop of the growing recognition of the practical importance of language and reasoning that familiarity with the liberal arts emerged as a modern notion of literacy. These skills became an expectation of any active participant in civil society.

Unfortunately, the ability to make things as well as ideas didn’t make the cut; that was relegated to the artes illiberales, the “illiberal arts,” that one pursued for mere economic gain. With art separated from artisans, the remaining fabrication skills were considered just mechanical production. This artificial division led to the invention of unskilled labor in the Industrial Revolution.

As with the revolution in painting in the Renaissance, this transition in industry was triggered in part by advances in materials, in this case the use of iron and steel, which in turn both led to and benefited from the development of steam power. These developments in materials and power made possible modern machinery, most notably mechanized looms. These could produce much more cloth than traditional artisans could (from 50,000 pieces in 1770 England to 400,000 pieces in 1800), and thus could clothe many more people (from 8.3 million people in 1770 England to 14.2 million in 1821). Newly unemployed craft workers crowded into growing cities to seek employment operating the machines that would replace not only the jobs but also the skills of still more workers. Unintended consequences of this shift included a layer of black smoke covering earth and sky, generated from burning coal in the factories, and the epidemics of cholera, smallpox, typhoid, typhus, and tuberculosis that followed from packing people around the factories.

This new division of labor between people and machines became explicit with Joseph-Marie Jacquard’s invention of the programmable loom, first demonstrated in Paris in 1801. He introduced an attachment that could read instructions on punched cards (more reliably than Florida’s voters) to control the selection of shuttles containing colored threads and thereby program patterns into fabric.

Because the looms could now follow instructions, their operators no longer needed to. The job of the weaver was reduced to making sure that the loom was supplied with thread and cards. Lyon’s silk weavers, threatened by this challenge to their livelihood, rather reasonably destroyed Jacquard’s looms. But the looms won; commercial weaving turned from a skilled craft into menial labor.

The invention of industrial automation meant that a single machine could now make many things, but it also meant that a single worker who used to do many things now did only one. Thinking about how to make things had became the business of specialized engineers; the Ecole Polytechnique was set up in France in 1794 to train them, and in Britain there was an unsuccessful attempt to forbid the export of both its engineers and the machines they developed because of the perceived strategic importance of both.

Tellingly, in Britain, where the separation between art and artisans was furthest along, scientific progress suffered. The great acoustics discoveries of the nineteenth century occurred in France and Germany, where there was a lively exchange in workshops that made both musical and scientific instruments, rather than in England, where handwerk had become a pejorative term.

From there, the relative meaning of literacy diverged for machines, their designers, and their users. First, the machines. Around 1812, the mathematician Charles Babbage conceived that it would be possible to construct a machine to do the tedious job of calculating mathematical tables, and in 1823 he received government support to build his “Difference Engine.” He failed to finish it (Babbage was also a pioneer in bad management), but the Difference Engine did produce one very useful output: the inspiration for the Analytical Engine. Babbage realized that an improved steam-powered engine could follow instructions on Jacquard’s punched cards to perform arbitrary mathematical operations, and could change the operation by changing the cards rather than the machine. By the mid-1830s Babbage had failed to complete this new machine as well, limited by the timeless problems of underfunding and mismanagement, and by the available manufacturing technology that could not make parts with the complexity and tolerances that he needed.

Jacquard’s punched cards reappeared in 1882 when Herman Hollerith, a lecturer at MIT who had worked for the U.S. Census Bureau as a statistician, sought a way to speed up the hand-tallying of the census. He realized that the holes in the cards could represent abstract information that could be recorded electrically. The result was the Hollerith Electric Tabulating System, which counted the 1890 census in a few months rather than the years that a hand tally would have required. The greater consequence of this work was the launch in 1896 of his Tabulating Machine Company, which in 1924 became IBM, the International Business Machines Corporation.

Information-bearing punched cards made machines more flexible in what they could do, but that didn’t change anyone’s notion of the nature of people versus machines. That challenge surfaced in an initially obscure paper published by the twenty-four-year-old Alan Turing in Cambridge in 1936. In “On Computable Numbers, with an Application to the Entscheidungsproblem,” he tackled one of the greatest outstanding mathematical questions of his day, the Entscheidungsproblem (“decision problem”) posed by the great mathematician David Hilbert in 1928: can there exist, at least in principle, a definite procedure to decide whether a given mathematical assertion is provable? This is the sort of thing that someone like Turing, employed in the business of proving things, might hope to be possible. His rather shocking answer was that it wasn’t. Alonzo Church, who would become Turing’s thesis adviser at Princeton, independently published the same conclusion in 1936, but Turing’s approach was later considered by everyone (including Church) to be much more clever.

To make the notion of “procedure” explicit, Turing invented an abstract mechanism that he called an LCM, a logical computing machine (everyone else just called it a Turing machine). This device had a paper tape that could contain instructions and data, and a head that could move along the tape reading those instructions and interpreting them according to a fixed set of rules, and could then make new entries onto the tape. This sort of machine could follow a procedure to test the truth of a statement, but Turing was able to show that simple questions about the working of the machine itself, such as

whether or not it eventually halts when given a particular set of instructions, cannot be answered short of just watching the machine run. This means that it might be possible to automate the solution of a particular problem, but that there cannot be an automated procedure to test when such an approach will succeed or fail.

This dramatic conclusion set a profound limit on what is knowable. The seemingly steady advance of the frontiers of knowledge had halted at a fundamentally unanswerable question: the undecidability of testing a mathematical statement. But Turing’s solution contained an even greater consequence: He showed that the particular details of the design of the Turing machine didn’t matter, because any one of them can emulate any other one by putting at the head of its tape a set of instructions describing how the other one works. For instance, a Mac Turing machine can use a PC Turing machine tape by starting it off with a PC specification written in Mac language. This insight, now called the Church-Turing thesis, is the key to machine literacy. Any machine that can emulate a Turing machine can solve the same problems as any other, because it is general enough to follow machine translation instructions. This property has since been shown to be shared by systems ranging from DNA molecules to bouncing billiard balls.

Turing’s thoughts naturally turned to building such a universal computing device; in 1946 he wrote a “Proposal for Development in the Mathematics Division of an Automatic Computing Engine (ACE)” for the UK’s National Physical Laboratory (NPL). Like Babbage, Turing proved himself to be better at proposing machines than building them, but those machines of course did get built by his successors. The story picks up across the Atlantic, where, shades of Babbage’s math tables, the U.S. Army funded the construction of an all-electronic machine to be built with vacuum tubes to calculate artillery range tables. The ENIAC (Electronic Numerical Integrator and Computer) was publicly dedicated at the University of Pennsylvania in 1946. Its first calculations weren’t range tables, though. They were something much more secret and explosive: mathematical models for the nuclear bomb effort at Los Alamos. Those calculations arrived via John von Neumann; getting him interested in computers was perhaps ENIAC’s

most important consequence. Von Neumann is on the short list of the smartest people of the past

century; those who knew him might say that he is the short list. He was a math wizard at Princeton, where he overlapped with Turing, and he was an influential government adviser. When von Neumann heard about the ENIAC through a chance encounter he planted himself at the University of Pennsylvania, recognizing how much more the ENIAC could do than calculate range tables. It was, after all, the first general- purpose programmable digital electronic computer.

It was also a rather clumsy first general-purpose programmable digital electronic computer. It tipped the scales at a svelte thirty tons, and for maximum operational speed it was programmed by plugboards that took days to rewire. It’s charitable to even call it programmable. But his experience with this computer did lead von Neumann to propose to the Army Ordnance Department in 1945 construction of the EDVAC (Electronic Discrete Variable Computer), and in 1946 he elaborated on this idea in a memo, “Preliminary Discussion of the Logical Design of an Electronic Computing Instrument.” He made the leap to propose that programs as well as data could be stored electronically, so that the function of a computer could be changed as quickly as its data. He proved to be a better manager than Babbage or Turing; the EDVAC was finished in 1952 (although the first stored-programs computers became operational at the universities of Manchester and Cambridge in 1949).

Having invented the modern architecture of computers, von Neumann then considered what might happen if computers could manipulate the physical world outside of them with the same agility as the digital world inside of them. He conceived of a “universal constructor,” with a movable head like a Turing machine, but able to go beyond making marks to actually move material. Because such a thing was beyond the capabilities of the barely functional computers of his day, he studied it in a model world of “cellular automata,” which is something like an enormous checkerboard with local rules for how checkers can be added, moved, and removed. Von Neumann used this model to demonstrate that the combination of a universal constructor and a universal computer had a remarkable property: self-

reproduction. The computer could direct the constructor to copy both of them, including the program for the copy to make yet another copy of itself. This sounds very much like the essence of life, which is in fact what von Neumann spent the rest of his life studying. I’ll return to this idea in “The Future” to look at the profound implications for fabrication of digital self-reproduction.

While von Neumann was thinking about the consequences of connecting a universal computer to machinery to make things, the first general-purpose programmable fabricator was actually being built at MIT. The Whirlwind computer was developed there in the Servomechanism Laboratory, starting in 1945 and demonstrated in 1951. Intended for the operation of flight simulators, the Whirlwind needed to respond to real-time inputs instead of executing programs submitted as batch jobs. To provide instantaneous output from the computer, the Whirlwind introduced displays screens. But if the computer could control a screen, that meant that it could control other things in real time. At the request of the air force, in 1952 the Whirlwind was connected to an industrial milling machine. The mechanical components for increasingly sophisticated aircraft were becoming too difficult for even the most skilled machinists to make. By having the Whirlwind control a milling machine, shapes were limited only by the expressive power of programs rather than by the manual dexterity of people. The machines of Babbage’s day weren’t up to making computers, but finally computers were capable of making machines. This in turn raised a new question: How could designers tell computers how to make machines?

The answer was the development of a new kind of programming language for doing what became known as computer-aided manufacturing (CAM) with numerically controlled (NC) machines. The first of these, Automatically Programmed Tools (APT), ran on the Whirlwind in 1955 and became available on IBM’s 704 computer in 1958. It was a bit like the theoretical programs for a Turing machine that would specify how to move the read/write head along the abstract tape, but now the head was real, it could move in three dimensions, and there was a rotating cutting tool attached to it. APT is still in use, and is in fact one of the oldest active computer languages.

APT was a machine-centric representation: it described steps for the milling machine to follow, not results that a designer wanted. Real computer aid on the design side came from the next major computers at MIT, the TX-0 and TX-2. These were testbeds for computing with transistors instead of vacuum tubes, and sported a “light pen” that allowed the operator to draw directly on the display screen. In 1960 Ivan Sutherland, a precocious student supervised by Claude Shannon (inventor of the theory of information that forms the foundation of digital communications), used the combination of the TX-2 and the light pen to create the seminal “Sketchpad” program. Sketchpad let a designer sketch shapes, which the computer would then turn into precise geometrical figures. It was the first computer-aided design (CAD) program, and remains one of the most expressive ones.

The TX-2 begat Digital Equipment Corporation’s PDP (Programmed Data Processor) line of computers, aimed at work groups rather than entire organizations. These brought the cost of computing down from one million dollars to one hundred thousand and then ten thousand dollars, sowing the seeds for truly personal computing and the PCs to come. The consequences of that personalization of computation have been historic. And limited.

Personal computers embody centuries of invention. They now allow a consumer to use a Web browser to buy most any product, from most anywhere. But online shopping is possible only if someone somewhere chooses to make and sell what’s wanted. The technology may be new, but the economic model of mass production for mass markets dates back to the origin of the Industrial Revolution.

Unseen behind e-commerce sites are the computers that run industrial processes. Connecting those computers with customers makes possible what Stan Davis calls “mass customization,” allowing a production line to, for example, cut clothes to someone’s exact measurements, or assemble a car with a particular set of features. But these are still just choices made from among predetermined options. The real expressive power of machines that make things has remained firmly on the manufacturing rather than the consumer side.

Literacy has, if anything, regressed over time to the most minimal meaning of reading and writing words, rather than grown to

encompass the expressive tools that have come along since the Renaissance. We’re still living with the historical division of the liberal from the illiberal arts, with the belief that the only reasons to study fabrication are for pure art or profane commerce, rather than as a fundamental aspect of personal liberation.

The past few centuries have given us the personalization of expression, consumption, and computation. Now consider what would happen if the physical world outside computers was as malleable as the digital world inside computers. If ordinary people could personalize not just the content of computation but also its physical form. If mass customization lost the “mass” piece and become personal customization, with technology better reflecting the needs and wishes of its users because it’s been developed by and for its users. If globalization gets replaced by localization.

The result would be a revolution that contains, rather than replaces, all of the prior revolutions. Industrial production would merge with personal expression, which would merge with digital design, to bring common sense and sensibility to the creation and application of advanced technologies. Just as accumulated experience has found democracy to work better than monarchy, this would be a future based on widespread access to the means for invention rather than one based on technocracy.

That will happen. I can say this so firmly because it is my favorite kind of prediction, one about the present. All of the technologies to personalize fabrication are working in the laboratory, and they are already appearing in very unusual but very real communities of users outside the laboratory. The stories of these technologies, and people, are the subject of the rest of this book.



## ~~The Present~~
There’s a demand for personal fabrication tools coming from
community leaders around the world, who are embracing emerging
technology to help with the growth of not only the food and the
businesses in their communities but also the people. This combination
of need and opportunity is leading them to become technological
protagonists rather than just spectators.

A natural next step was to set up a fab lab at SETC, aimed at
democratizing not just the use of technology but also its development.
As the machines arrived, hands-on training began with hello-world
examples (of course), cutting and printing text with the tools. These
were intended to be no more than exercises before the serious work
started, but there turned out to be much more interest in them in their
own right. The ability to make a tangible hello-world represented a real
opportunity to personalize fabrication.

People, particularly kids, at SETC put personally meaningful words
and images on anything they could get their hands on: jewelry, shoes,
tools, and more. The lab began producing custom pop-up greeting
cards, and dioramas, and “Adinkra” symbols sent from the Ghana fab
lab. All of this activity was like a chorus of technological voices saying
“I’m me. I exist.”

Community access to the precision fabrication tools in Mel’s fab lab
turned shipping boxes from trash into a valuable raw material that
could be remanufactured into something else, like the high-tech crafts
the girls at SETC sold, with implications ranging from personal
empowerment to communal recycling to urban economic opportunity.
These are big things in communities that have little hope. As Mel says,
“The rear wheels of the train don’t catch up to the front wheels of the
train unless something happens to the train.
” At SETC, personal
fabrication offers an opportunity to speed up the back of the train.

Anil runs the “Honeybee Network.
” Modeled on how honeybees work
—collecting pollen without harming the flowers and connecting flowers
by sharing the pollen—the Honeybee Network collects and helps
develop ideas from grassroots inventors, sharing rather than taking
their ideas. At last count they had a database of ten thousand
inventions.

## ~~Description~~

Personal computing hardware requires software programs in
order to do anything. Likewise, personal fabrication hardware requires
a description of a design in order to be able to make anything. Those
designs could be bought online and downloaded like the songs that go
into a portable music player, but the promise of personal fabrication
goes beyond consumption to invention. This entails the use of software
for computer-aided design (CAD). The purpose of these design tool
programs is to help capture what might be an elusive conception of
something and convert it into a precise specification for construction.
Physical fabrication tools don’t (yet) have an undo button; once an
operation is performed on an object it’s hard to retract it. At best, CAD
software assists with the development of an idea by providing an
environment for easy experimentation before fabrication. At worst,
CAD software can be infuriatingly difficult to use and impose
constraining assumptions about what will be made, and how.
The most affordable, and in many ways versatile, engineering
design tool costs around ten cents: a pencil. This often-underrated
instrument has a number of desirable features. Its user interface is
easy to understand, the response to commands is immediate, it’s
portable, and it creates high-resolution images. None of the more
advanced software tools to come in this chapter can match all of that.
For this reason, doodling is an invaluable ingredient in advanced
design processes. It can help develop concepts before they get frozen
by the vastly more capable but less flexible computerized tools. And
sketches can be scanned into a computer as a starting point in the
design process, or be sent directly to a fabrication machine as a 2D
toolpath.
Beyond ten cents, it’s possible to spend thousands and even
millions of dollars on design software and the computers to run it. This
rarely buys more inspiration. Quite the contrary, actually—the learning
curve of high-end engineering design tools can feel more like walking
into a cliff rather than up a hill, with an enormous amount that needs to
be learned before it’s possible to do anything. What serious CAD tools
do bring is the ability to manage much more complex projects. As the
number of parts (and people) involved in a design grows, keeping
track of what goes where, and who is doing what, becomes an
increasingly onerous task. Industrial-strength software goes beyond
design to manage workflow, from the initial concept all the way through
to generating lists of parts to order and instructions to control the
production machines, along with controlling the marketers and lawyers
and everyone and everything else involved in the life cycle of a product.
T o see how this process works (the design, not the lawyer part), let’s
start with the ten-cent solution:
This hello-world sketch took just ten seconds to produce, and in
making it, I had available a choice of fonts and line styles limited only
by my nearly nonexistent drawing ability.
The problem with drawing on a piece of paper comes when it’s time
to make changes, or to express more complex relationships in a
drawing. There are just two operating modes: adding marks with the
pencil, and removing them with the eraser. Drawing software adds the
ability to constrain and modify as well as add and remove marks.
Bitmap- or pixel-based drawing programs manipulate an image just
as a piece of paper does, by recording how much electronic ink is
stored in each part of the page. Bitmap refers to the representation of
an image as an array of data bits; a pixel is a picture element in such
an image, where turning the c into an x saves the word from sounding
like a deli food. Pixel programs are used for expressive things like
illustrations, but are less useful for technical drawing. This is because a
pixel-based hello-world
is made up of many dots:
Once these dots are drawn, they no longer know that they’re part of an
“e”; the only way to change the letter is to erase it and start over.
In contrast to drawing with pixels, a vector-based drawing program
effectively saves what you do rather than what the piece of paper
does. These programs store a line by recording the location of the end
points, thereby defining a vector, and more generally they store shapes
by recording the points used to define the shapes, such as the center
and radius of a circle. This allows an object to be later moved or
modified by changing its reference points. And since the shapes are
defined by mathematical functions rather than dots, they can be drawn
with as much detail as is needed for the output from the CAD
environment, whether it is being used to cut steel or place atoms. Most
important of all, these shapes can be combined with others to build up
more complex structures.
A vector hello-world looks the same as a pixel hello-world from a
distance:
but it’s now possible to zoom in
without losing resolution because the letters are made up of curves
rather than dots. Here are the points that define those curves:
Since mathematical functions define the shapes of the letters,
mathematical mappings can be applied to them. One particularly
useful set of editing operations is the Boolean transformations, named
after the mathematician George Boole, who in 1854 formulated a
mathematical representation of logic. For example, we could overlay a
base rectangle on our hello-world:
and then use Boolean operations to add (top image, below),
subtract (middle image), or intersect (bottom image) them:
After pixels and vectors, the next distinction in computer drawing
programs is between those that can work with 2D shapes and those
that work with 3D solids. Just as there are 2D fabrication tools, like a
laser cutter, and 3D ones, like an NC mill, there are CAD programs
targeted at 2D and at 3D design. In a 3D CAD program, we can draw
our hello-world again,
but now rotate it,
and then extrude it to form 3D letters:
Once again, we can introduce a pedestal, now three-dimensional,
and then use 3D Boolean operations to add (top image, below),
subtract (middle image), or intersect (bottom image) the letters with it:
T o make the last image easier to interpret, the computer simulated
the light and shadow by using a mathematical model of a lamp
illuminating the solids. This process, called rendering, is an important
part of developing design.
Beyond the transition from two to three dimensions, the real aid in
computer-aided design comes in programs with a set of capabilities
loosely called parametric. These programs specify the essential
aspects of a design in a way that lets them later be modified. Let’s say
you’re designing a spaceship. It would take an awful lot of mouse
clicks to draw in every nut and bolt. After you’re done, if you find that
the bolts are not strong enough to hold the spaceship together, you
would then need to laboriously change them. If instead you define an
archetypal bolt, with its diameter specified as an adjustable
parameter, and then if this master bolt is duplicated all over the design,
then changing that single parameter will modify all of the instances of
the bolt. Furthermore, you can define a bolt hole to have what’s called
an associative relationship with a bolt, in this case leaving a desired
amount of space around it, and the nut and bolt can be grouped
together to form a hierarchical assembly, so that now changing the
original bolt automatically enlarges the hole around it, and those
changes carry through to all of the nutand-bolt assemblies duplicated
in the design.
As an example of parametric modeling, here’s a plate with some
holes:
These holes were made in a parametric CAD program by drawing a
cylinder, making three copies of it, moving the cylinders to the corners
where the holes are to go, and then doing a Boolean subtraction of all
of them from the plate to create the holes. Because the program
remembers these operations and the relationships they defined,
changing the diameter of the original cylinder automatically updates all
the holes:
Now, it would be rather awkward to discover that a spaceship’s
bolts were not strong enough once you’re on your way to Mars, so still
more capable CAD programs add the ability to model the behavior as
well as the shape of a part. Using the mathematical technique of finite
element analysis (FEM), it’s possible to check how a design will
respond to most anything: forces, vibration, heat, airflow, fluid flow, and
electric and magnetic fields. Finite element analysis works by covering
a structure with a mathematical mesh of points
and then calculating how those points move in response to applied
forces, here pushing up in the middle:
The shading corresponds to the calculated local strain
(displacement) in the material, with lighter areas moving more. This
calculation provides qualitative insight into where the part should be
strengthened, and quantitative information as to whether the strain is
so large that the material will crack. Here it can be seen that the area
around the top of the W is a likely failure location.
Even with all the enhancements, these examples of CAD tools still
share a particularly serious limitation: they fail to take full advantage of
the evolution of our species over the past few million years. The human
race has put a great deal of effort into evolving two hands that work in
three dimensions, with eyes to match, but the interface to these design
tools assumes only one hand that works in two dimensions (dragging a
mouse around a desktop). A frontier in CAD systems is the
introduction of user interfaces that capture the physical capabilities of
their users. These range from knobs and sliders that can control a
three-dimensional graphical view, to data gloves that can convert the
location of your hands into the location of on-screen hands that can
directly manipulate a computer model, to whole rooms with displays
projected on the walls that make it possible to go inside a computer
model. The most interesting approach of all is to abandon the use of a
computer as a design tool and revert to the sophisticated modeling
materials used in a well-equipped nursery school, like clay:
An image of the clay can be sent to a two-dimensional cutting tool,
here using the contours around the letters as a toolpath for a sign
cutter to plot them in a copper sheet:
The same file could be cut in acrylic on a laser cutter, or in glass or
steel on a waterjet cutter. Alternatively, a three-dimensional “picture” of
the clay could be taken by using a 3D scanner:
There’s a laser in the scanner on the right that’s moving a spot back
and forth over the surface of the clay. The scanner uses a camera to
watch where the laser light is reflected back to it, and from those
measurements it can deduce the shape of the surface that’s scattering
the laser beam. The result is a computer model of the clay:
These shapes would have been difficult to define from scratch as a
mathematical model, but once scanned and imported into a CAD
environment they can be manipulated like any other object: scaled
larger or smaller, bent around a surface, or merged with other
components by Boolean operations. The resulting model can then be
output on a fabrication tool such as a 3D printer or an NC mill,
effectively converting the clay into plastic or steel, with enhancements
added through the CAD editing. Used this way, the computer assists
with the development of a design but it isn’t expected to be the
medium in which the original inspiration happens.
In the end, there is no single best kind of CAD. Software design
tools are really much like any other kind of tool. No one tool, hardware
or software, can do all jobs equally well. Just as a serious artist works
surrounded by a variety of paints and brushes, serious computer-aided
design entails working with a collection of programs. Like the tools on
a workbench, there are familiar favorites with well-worn ways of
working, less-used special-purpose programs, and the occasional
custom program needed for a particular task.
Ultimately, the development of universal programmable personal
digital fabricators may drive the development of correspondingly
universal personal digital design software. Until then, CAD will remain
a fundamentally enabling, occasionally exasperating, and altogether
essential part of personal fabrication.

## ~~Problem solvers~~

As work in the fab lab progressed from teaching tutorials to working
on real-world applications, it ran into a significant limitation. One of the
highest priorities to quickly emerge in the Ghana fab lab was solar
energy, seeking to develop machines that could directly harness the
abundant power of concentrated sunlight without the cost and
inefficiency of converting it to electricity first. This activity required
making solar collectors that were not only bigger than the cutting tools
in the fab lab but needed to be bigger than the lab itself. But how can a
machine make something bigger than itself?
An answer can be found all the way back in Seymour Papert’s turtle.
The fab lab could make a mobile computer-controlled car that could
drive over the material to be used, trailing a pen to plot precise shapes
that could then be cut out with simple hand tools. The original turtles
provided graphical feedback long before computer graphics were up
to the job; this new kind of turtle could do the same for fabrication. A
project soon followed to develop a turtle design that could be produced
in the fab lab.
This solution to the practical need to plot large structures represents
the realization of one of Seymour Papert’s original dreams. As exciting
as bringing computers and controllers into the classroom once was, he
describes the inability of the kids back then to invent as well as use
technology as a “thorn in our flesh.
” Unlike what’s possible with the
materials in any well-equipped arts-and-crafts area, the turtle provided
output from the computer but the physical form of the turtle itself
couldn’t be changed. Its fixed form put a severe bound on the shape of
kids’ ideas.
The possibility now of making a turtle in a fab lab is important as an
application, allowing the production of structures bigger than the lab.
The implications range all the way up to global energy economics in
fabricating large-scale solar collectors. But a do-it-yourself turtle is
even more important as a process, breaking down the barrier between
using and creating technological tools. Like real turtles, one starting
design can evolve into many subspecies. The inspiration for such an
invention can be play, or work, and, best of all, because it can be done
by an individual rather than justified by an organization it’s not
necessary to try to tell the difference.

## ~~Network~~

If the practice of personal fabrication expresses a deep-seated
human desire to create, a driving inspiration for that creation is
communication. Around the world, one of the first things people do
when they get access to tools for technological development is to
apply them to accessing and exchanging information. This leads to
some unexpected innovators in telecommunications infrastructure.

## ~~Interaction~~

Unlike an obedient child, most machines are meant to be heard as
well as seen, or in some other way to alter their environment. This last
of the tools chapters looks at some of the ways that things can
communicate with people.

## ~~Art & Artillery~~

Corporations sell to consumers as individuals, but individuals live
in communities. And those communities can have compelling
collective technological needs that are not met by individual
acquisitions, notably in the ways groups of people can access and
interact with information.
The unsuitability of computers that are designed for a person but
need to be used by a group has led community leaders around the
world to take on technological development projects to create
computers that are appropriate for their world. In some of the most
beautiful, and dangerous, places on the planet, these efforts are
literally as well as figuratively on the front lines of a kind of fabrication
that is not just personal, it’s essentially social.
Terry
T erry Riley is the chief curator for architecture and design at New
Y ork’s Museum of Modern Art. About once a decade MoMA mounts a
landmark architectural exhibition that takes stock of the kinds of
buildings being built. In 1998 T erry contacted me with a problem: For
an upcoming show there was a great deal of new digital media to
include, but he was looking for an alternative to the use of conventional
computers in the galleries to access this information. MoMA goes to
great effort to create a compelling visual and social space in the
museum; he didn’t want to destroy that with the solitary experience of
hunching over a computer to type on its keyboard. Terry asked whether
the furniture in the gallery could itself be used as an information
interface.
Now, designing the details of computer interfaces is more typically
the rather dry business of computer scientists and electrical engineers,
but in this case T erry felt that those engineers weren’t considering how
groups of people rather than individuals could use computers, in a
beautiful setting, without any instruction. So, with the show looming, he
and his colleagues took it upon themselves to piece together a
graceful, unobtrusive communal computer.
Lab space
We assembled a team of MoMA curators and MIT students to work
on the project. The team put together prototype components at MIT ,
then we all decamped to midtown Manhattan to set up shop in the
gallery for the final development. This was a striking, and challenging,
site for a working lab.
The collaborative development process settled on a giant
diningroom table as the focus of the exhibit. This had displays
projected on it, and sensors in it to detect people as well electronic
tags in objects. The goal was to retain the metaphor of architects
looking at blueprints, but make the blueprints active. A central lazy
Susan was dotted with what looked like coasters for drinks, each
containing an image of a project in the show; as these tangible icons
were brought near a place setting, the associated blueprint would
magically appear (thanks to readers in the table detecting the tags in
the coasters). Then the table would sense the viewers’ hands, allowing
museum visitors to browse through images and videos embedded in
the plans. Particularly interesting pictures could be electronically slid
onto the lazy Susan and shared around the table.
Experiencing art
“No computers”
The designing and debugging continued right up to the opening of
the show. As the minutes ticked by, we didn’t realize that a few
galleries away a wall of guards was holding back a sea of humanity.
The guards parted, the sea flowed, we were swept away, and a crowd
congregated around the table. We weren’t sure how well, or even if, the
table was working until an elderly museum benefactor came shuffling
out of the crowd. Beaming, she said,
“I hate computers, but this is
great because there are no computers here!”
The museum benefactor didn’t realize that the table she was
pounding on housed seventeen Internet-connected embedded
computers, communicating with hundreds of sensor microcontrollers
covering the bottom surface of the table. But she was also right, in that
there were no visible computers. By bringing that much processing
power that close to people, the computing literally disappeared into the
woodwork. Neither she nor I could tell exactly which device was doing
what. The multiple tag readers, hand detectors, lazy Susan sensors,
video projectors, and information databases were all interacting over
the Internet. The table was not a peripheral to a computer; it was an
essential part of MoMA’s infrastructure for the show.
The table solved one problem—the intrusion of computers into the
gallery—but caused another one: Some of the museumgoers were
more interested in the interface than in the rest of the show. Although
MoMA isn’t (yet) in the business of technology development, visitor
after visitor asked where they could buy an interactive table for use in
their communal computing setting: teaching, or business management,
or financial investing, or emergency services, or military command and
control. The creative demands of exhibiting art had inspired the
creation of a computer interface that was applicable far beyond a
gallery.

## ~~The Future~~

The liberal arts were originally meant to liberate, empowering
through mastery of the means of expression. The new means of
expression for personal fabrication offer technological empowerment,
providing economic, intellectual, and even a kind of spiritual liberation.
I believe that the best place to read about the future of personal
fabrication is in the faces of a new generation getting access to
prototype versions of these capabilities in fab labs. Coming from many
different parts of the planet, they end up in the same place: joy.

